# LLM Hypothesis Testing Benchmark - ht-bench

A research benchmark evaluating LLMs on statistical hypothesis testing tasks across 16 models, 4 prompting strategies, and 3 T-test types.

## ğŸ¯ Key Findings

- **Top performers**: Gemini 2.5 Pro (85.5%), Grok-3 (82.4%)
- **Critical insight**: "Outcome-Process Dissociation" â€” models achieve 98.7% sensitivity but fail on test statistic derivation
- **Prompting paradox**: Program-of-Thought > Few-Shot > Zero-Shot > Chain-of-Thought
- **Systematic weakness**: Paired T-Test accuracy drops ~30% vs other tests

## ğŸš€ Quick Start

```bash
# Install
pip install -r requirements.txt
.env  # Add API keys

# Run benchmark
python ht.py --mode quick          # Fast test (1 model, 2 scenarios)
python ht.py --mode full           # All models, all tests
python ht.py --mode custom --models openai/gpt-4o --tests one_sample_t_test --scenarios 5

## ğŸŒ Live Demo

Try the interactive dashboard: **[ht-bench.streamlit.app](https://ht-bench-alfietry.streamlit.app)**

# View results
streamlit run dashboard/app.py
```

## ğŸ“ Architecture

```
ht.py                 â†’ Orchestrator (async batch evaluation)
â”œâ”€â”€ data_generator.py â†’ Synthetic scenarios (t-tests, sample sizes)
â”œâ”€â”€ prompts.py        â†’ 4 strategies: zero_shot, few_shot, cot, pot
â”œâ”€â”€ llm_clients.py    â†’ OpenAI, Anthropic, Google, Grok, DeepSeek
â”œâ”€â”€ response_parser.pyâ†’ JSON/regex extraction cascade
â”œâ”€â”€ statistical_engine.py â†’ SciPy ground truth
â””â”€â”€ evaluator.py      â†’ Metrics (accuracy, reasoning, hallucination)
         â†“
results/*.json â†’ dashboard/app.py (Streamlit visualization)
```

## ğŸ“Š Evaluation Metrics

| Metric | Description |
|--------|-------------|
| Overall Accuracy | Mean of test-method, p-value, decision accuracy |
| Decision Accuracy | Correct reject/fail-to-reject at Î±=0.05 |
| P-value Accuracy | Within Â±0.05 tolerance |
| Reasoning Quality | Rubric score [0,1] for explanation quality |
| Hallucination Rate | Invalid values, contradictory decisions |

## ğŸ”§ Configuration

Edit `config.py`:
- `FULL_MODE_MODEL_MAP`: Models for full benchmark
- `EVALUATION["p_value_tolerance"]`: Default 0.05
- `RANDOM_SEED = 42`: Reproducibility

## ğŸ“ Adding Components

**New LLM provider:**
1. Subclass `LLMClient` in `llm_clients.py`
2. Add to `config.py` API_KEYS and MODELS
3. Register in `get_client()` factory

**New statistical test:**
1. Add generator in `data_generator.py`
2. Add computation in `statistical_engine.py`
3. Register in `config.TEST_TYPES`

## ğŸ“„ License

For educational and research purposes.