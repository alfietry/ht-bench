\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{hyperref}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Benchmarking Large Language Models on Statistical Hypothesis Testing}

\author{\IEEEauthorblockN{Alfred K. Adzika}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Ohio University}\\
Athens, OH, USA \\
email@ohio.edu}
}

\maketitle

\begin{abstract}
We study whether modern Large Language Models (LLMs) can reliably carry out statistical hypothesis testing. We present a reproducible benchmark that performs hypothesis testing as a holistic capability: selecting an appropriate test, computing statistics and p-values, and making decisions under a significance level. The framework evaluates models under multiple prompting strategies (Zero-Shot, Few-Shot, and Chain-of-Thought), uses synthetic data with known ground truth, and reports standardized metrics (overall accuracy, decision accuracy, p-value tolerance, reasoning quality, latency). CoT improves structure but remains vulnerable to arithmetic errors and completeness. We discuss strengths, limitations, and paths toward trustworthy AI systems that can participate in evidence-based scientific workflows leading to justifiable new discoveries.
\end{abstract}

\begin{IEEEkeywords}
Large Language Models, Hypothesis Testing, Benchmarking, Scientific Reasoning, AI Evaluation
\end{IEEEkeywords}

\section{Introduction}
This work is a study in artificial intelligence, guided by a long-term aim of understanding how AI systems could help uncover new domains of knowledge. \cite{wang2023scientific}. At its core, the project investigates one essential capability that underlies all scientific progress: hypothesis testing. Specifically, we study how well large language models (LLMs) can evaluate the validity of claims under controlled conditions.

In scientific research, progress depends not only on generating ideas, but also on the ability to test, validate, refine, and reject them. While LLMs have shown impressive performance in tasks such as text generation, reasoning, and problem-solving \cite{openai2023gpt, wei2022chain}, there is still limited systematic understanding of how well they perform at evaluating the correctness of claims in a way that resembles true hypothesis testing. If LLMs are ever to meaningfully support scientific discovery, they must be able not just to propose ideas, but to rigorously assess whether those ideas hold up under evidence and structured testing.

This project serves as groundwork toward measuring that capability. Our central objective is to design and implement a benchmark that evaluates how effectively modern LLMs can perform hypothesis testing. By formalizing hypothesis testing as a measurable task for language models, we aim to provide a standardized way to quantify how accurately these systems can judge the validity of claims, weigh evidence, and reach reliable conclusions. Concretely, we determine success around consistent test selection, calibrated p-value estimation, and correct reject/fail-to-reject decisions at a fixed $\alpha$.

In the long term, this line of work connects to a much larger question: if future AI systems are to assist in the formation of new areas of study, then they must first demonstrate reliable performance in evaluating the foundational claims upon which new fields are built. This research does not attempt to establish new fields directly; rather, it focuses on building an evaluation infrastructure needed to assess whether LLMs are developing the kinds of reasoning and validation skills that such discovery would require.

Accordingly, this work will contribute:
\begin{itemize}
    \item A formal benchmark for hypothesis testing with LLMs,
    \item An empirical evaluation of LLMs on this benchmark,
    \item And an analysis of their strengths, and limitations.
\end{itemize}

\section{Related Work}
A growing body of work has focused on evaluating the logical reasoning capabilities of large language models. Benchmarks such as LogicBench \cite{parmar2024logicbench} and LogicAsker \cite{wan2024logicasker} systematically test LLMs on propositional and first-order logic, revealing that while modern models can solve surface-level inference tasks, they still struggle with deeper multi-step and rule-consistent reasoning. Multi-LogiEval \cite{patel2024multi} tests LLMs on harder kinds of logical thinking involving doing many steps, handling facts that change old conclusions, and switching between different types of logic. As the tasks get deeper or require more steps, model performance drops sharply. Recent survey work on logical reasoning in LLMs \cite{liu2025logical} consolidates these findings and highlights persistent gaps in deductive reliability. While these efforts provide critical insights into formal reasoning, they do not directly evaluate statistical hypothesis testing as a reasoning task.

Beyond pure logical inference, recent studies have examined whether LLMs can evaluate the correctness of claims and their own reasoning. Work on self-verification in logical reasoning \cite{hong2024closer} shows that LLMs often fail to reliably detect their own logical inconsistencies. FactReasoner \cite{marinescu2025factreasoner} introduces a probabilistic framework for long-form factuality assessment, focusing on evidence-grounded verification of claims. Similarly, Trustworthy Reasoning \cite{jiao2025trustworthy} demonstrates that even when final answers are correct, intermediate reasoning steps frequently contain factual or logical errors. These works highlight the importance of explicitly assessing whether a claim is valid, but they focus mainly on factual accuracy and reasoning self-analysis, not on systematic statistical hypothesis testing.

More closely related to our work, several recent papers investigate whether LLMs can perform statistical reasoning from data. Liu et al. \cite{liu2024llms} propose a benchmark for quantitative and causal reasoning based on tabular and numerical data, showing that while LLMs can handle simple statistical patterns, they struggle with multi-variable dependencies and causal structure. Zhu et al. \cite{zhu2024are} systematically evaluate whether LLMs can function as reliable statisticians, testing tasks such as distribution estimation, hypothesis testing components, and statistical decision-making, and find that performance is highly unstable across test types and noise conditions.

Most directly aligned conceptually with our work, Tiwari \cite{tiwari2025framework} introduces a general framework for automated hypothesis testing using AI systems. Although this work proposes a procedural framework for automating the process, it does not offer a standardized, model-independent benchmark for empirically assessing LLM performance on hypothesis testing as a cohesive task.

\subsection{Gap \& Contribution}
Despite rapid progress in evaluating LLMs for logical reasoning, factual verification, and isolated statistical tasks, there is currently no standardized benchmark dedicated to evaluating hypothesis testing as a unified end-to-end capability of large language models on statistical t-tests problems. Existing reasoning benchmarks emphasize formal logic, while statistical studies primarily assess fragmented subtasks such as distribution estimation or numeric reasoning. Furthermore, automated hypothesis testing frameworks remain largely conceptual and lack large-scale empirical validation on modern LLMs.

This work fills that gap by proposing a dedicated benchmark to assess LLMs’ hypothesis-testing abilities as an integrated reasoning and computational task. We formally define hypothesis testing as an evaluable capability, construct a benchmark that covers structured claim formulation, statistical decision-making, and validity assessment, and perform an extensive evaluation of current LLMs. Our findings offers a systematic characterization of LLM strengths and weaknesses in statistical hypothesis testing, laying the groundwork for future research in AI-enabled scientific verification and discovery.

\section{Foundational Background}

\subsection{Hypothesis Testing}
Statistical hypothesis testing is a method of statistical inference used to decide whether the data at hand sufficiently support a particular hypothesis. It involves:
\begin{itemize}
    \item \textbf{Null Hypothesis ($H_0$)}: The default position that there is no relationship between two measured phenomena.
    \item \textbf{Alternative Hypothesis ($H_1$)}: The position that there is a relationship.
    \item \textbf{Test Statistic}: A standardized value calculated from sample data during a hypothesis test.
    \item \textbf{P-value}: The probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.
\end{itemize}
A decision is made to reject or fail to reject $H_0$ based on a significance level ($\alpha$), typically 0.05.

\subsection{Large Language Models \& Prompting}
LLMs are transformer-based models trained on vast corpora of text. Their performance on complex tasks can be significantly improved through prompting techniques:
\begin{itemize}
    \item \textbf{Zero-Shot}: Asking the model to perform the task without examples.
    \item \textbf{Few-Shot}: Providing examples of the task within the prompt.
    \item \textbf{Chain-of-Thought (CoT)}: Encouraging the model to generate intermediate reasoning steps \cite{wei2022chain}.
\end{itemize}

\section{Benchmark Design and Architecture}
\label{sec:design-architecture}

\subsection{System Overview}

The benchmark is implemented as a modular, asynchronous Python pipeline that evaluates large language models (LLMs) on end-to-end hypothesis testing. The main controller is the \texttt{HypothesisTestingBenchmark} class in \texttt{ht.py}, which handles five main subsystems: 
\begin{itemize}
    \item synthetic data generation
    \item prompt construction
    \item multi-provider LLM clients
    \item response parsing and statistical evaluation, and
    \item aggregation and visualization.
\end{itemize}
Each evaluation generates a structured JSON entry containing the scenario, prompt, model output, parsed components, ground-truth statistics, and computed metrics. These JSON records are subsequently tranfered to the Streamlit dashboard.

\subsection{Core Components}

\textbf{Data generation and ground truth} \\
Synthetic hypothesis-testing scenarios are created by the \texttt{DataGenerator} in \texttt{data\_generator.py}. It supports one-sample and two-sample $t$-tests, paired $t$-tests,  via type-specific generators. Scenarios are parameterized by sample size, distributional assumptions, and sample sizes. The \texttt{StatisticalEngine} computes the ground-truth test statistic, $p$-value, and decision for each scenario so that all models are compared against the same reference implementation.

\textbf{Prompt engine}\\
Prompt construction is handled by \texttt{prompts.py}, which defines a family of templates derived from a common \texttt{PromptTemplate} base class. The benchmark currently supports zero-shot, few-shot, and Chain-of-Thought (CoT). Each template takes the numeric scenario (samples, groups) and a test-specific natural language context from \texttt{create\_test\_context}, and renders a compact, model-facing input string. 

\textbf{LLM client abstraction}\\
All provider-specific details are encapsulated in subclasses of the abstract \texttt{LLMClient} interface (\texttt{llm\_clients.py}). Concrete implementations include \texttt{OpenAIClient}, \texttt{AnthropicClient}, \texttt{GoogleClient} (Gemini), and \texttt{GrokClient}, each normalizing differences in authentication, endpoints, and parameter conventions. The orchestrator in \texttt{ht.py} instantiates these clients from a simple configuration mapping of providers to model names, so that adding a new model typically requires only updating the configuration, not the pipeline logic.

\textbf{Response parsing and evaluation}\\
Raw LLM outputs are converted into a structured representation by \texttt{ResponseParser}. The parser first looks for the CoT \texttt{RESULT} line and, if present, populates a \texttt{ParsedResponse} Pydantic model with the test method, statistic, $p$-value, and normalized decision. If no such line exists, it falls back to a combination of JSON extraction (for structured modes) and regex-based extraction of hypotheses, test type, test statistic, $p$-value, degrees of freedom, decision, confidence interval, assumptions, and conclusion from free-form text. Validators on the Pydantic model enforce basic sanity checks (e.g., $p \in [0,1]$) and normalize decision strings to \texttt{reject\_H0} versus \texttt{fail\_to\_reject\_H0}. The \texttt{EvaluationMetrics} module then compares the parsed response against the ground truth and computes accuracy at several levels (test-type, $p$-value, decision, reasoning quality, hallucination flags, latency).

\textbf{Aggregation, storage, and visualization}\\
Each evaluation call returns a JSON-serializable record that includes a timestamp, model name, prompt type, test type, prompt text, raw response, parsed result, ground truth, evaluation metrics, and latency. The orchestrator in \texttt{ht.py} accumulates these records in-memory and periodically writes them to timestamped JSON files under \texttt{config.RESULTS\_DIR}. The \texttt{BenchmarkAggregator} in \texttt{evaluator.py} computes summary statistics by model, prompt type, and test type, as well as comparison matrices used in the report. The interactive dashboard in \texttt{dashboard/app.py} loads all JSON results, flattens them into a Pandas DataFrame, and exposes a leaderboard-style UI (built with Streamlit and Plotly) that presents per-model accuracy, confidence intervals, error breakdowns, and qualitative examples through dedicated analysis tabs.

\subsection{Execution Flow}
A benchmark run is configured and launched from \texttt{ht.py}, either in quick, full, or custom mode. For a given configuration, the orchestrator:

\begin{enumerate}
    \item Samples a grid of \emph{test types} and \emph{sample sizes} and calls \texttt{DataGenerator} to create synthetic scenarios with associated metadata.
    \item For each provider/model and each selected \emph{prompt type}, generates a natural-language prompt  using \texttt{get\_prompt} from \texttt{prompts.py}.
    \item Dispatches all (model, prompt, scenario) triples asynchronously via the appropriate \texttt{LLMClient}, respecting a global concurrency limit through an \texttt{asyncio.Semaphore}.
    \item Parses each returned response with \texttt{ResponseParser}, computes ground truth with \texttt{StatisticalEngine}, and evaluates correctness and auxiliary metrics with \texttt{EvaluationMetrics}.
    \item Appends the resulting evaluation record to the in-memory result list and, at the end of the run, writes the full set of records to disk and prints a textual summary using \texttt{BenchmarkAggregator}.
\end{enumerate}

This design cleanly separates concerns: data generation and ground truth are independent of model and provider; prompting strategies are independent to the LLM backends; and parsing/evaluation are shared across all models and modes.

\subsection{Extensibility and Reproducibility}

The modular architecture is explicitly designed for extensibility. New statistical tests can be added by implementing a generator method in \texttt{DataGenerator}, extending \texttt{create\_test\_context}, and adding the test to the orchestrator's list of supported types. New prompting strategies are added by defining a new \texttt{PromptTemplate} subclass and registering it in \texttt{get\_prompt}. Additional models or providers are integrated by subclassing \texttt{LLMClient} and extending the configuration mapping in \texttt{config.py}. Because all runs are parameterized through configuration files, fixed seeds in \texttt{DataGenerator}, and timestamped JSON outputs, the benchmark is reproducible and auditable: any reported result can be traced back to the exact synthetic scenario, prompt, raw response, and evaluation logic used.

\subsection{Evaluation Metrics}
\label{sec:evaluation-metrics}

For each tuple the benchmark logs a structured evaluation record. Core scalar metrics are:

\begin{itemize}
    \item \textbf{Test-method accuracy}: Indicator of whether the predicted test family (e.g., one-sample $t$-test, paired $t$-test) matches the ground truth.
    \item \textbf{Decision accuracy}: Indicator of whether the model’s reject / fail-to-reject decision at $\alpha=0.05$ agrees with the ground-truth decision.
    \item \textbf{P-value accuracy}: Indicator for whether the reported $p$-value lies within a fixed tolerance band of the true $p$-value and implies the same decision at $\alpha$.
    \item \textbf{Overall accuracy}: Mean of the three components above plus a consistency check between the reported statistic, $p$-value, and decision.
    \item \textbf{Reasoning quality}: A rubric-based score in $[0,1]$ capturing whether the response states hypotheses, identifies the test, explains the computation, and interprets the result correctly.
    \item \textbf{Hallucination flag}: Binary indicator of materially unsupported content (e.g., wrong test name for the data, self-contradictory decision, impossible numerical values).
    \item \textbf{Latency}: End-to-end model call duration in seconds, measured with \texttt{time.perf\_counter}.
\end{itemize}

These per-sample metrics are aggregated by model, prompt strategy, and test family (mean, standard error, confidence intervals) and drive the leaderboard, radar plots, heatmaps, and error analyses reported in Section~\ref{sec:evaluation-results}.

\section{Evaluation \& Results}
Results aggregate across the three $t$-test families currently enabled (one-sample, two-sample, and paired $t$-tests), two sample-size regimes ($n=20$ and $n=50$), and the four prompt strategies.

\subsection{Leaderboard: Overall Accuracy and Reliability}

The primary leaderboard view ranks models by mean overall accuracy with 95\% confidence intervals and exposes decision accuracy, hallucination rates, reasoning scores, and latency. Table~\ref{tab1} summarizes the aggregated results from the most recent full run used to populate the dashboard.

\begin{table}[htbp]
\caption{Model Performance Comparison (Dashboard Summary)}
\begin{center}
\small
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{N} & \textbf{Overall} & \textbf{Decision} & \textbf{Reasoning} & \textbf{Hal.} & \textbf{Latency} \\
      &  & \textbf{Acc.}    & \textbf{Acc.}     & \textbf{Score}     & \textbf{Rate}   & \textbf{(s)} \\
\hline
gemini-2.5-pro     & 111 & 85.5\% $\pm$ 3.7\% & 84.7\% & 0.73 & 10.8\% & 22.74 \\
gemini-2.5-flash   & 120 & 82.8\% $\pm$ 3.8\% & 87.5\% & 0.74 & 10.0\% & 17.49 \\
grok-3             & 356 & 82.4\% $\pm$ 2.4\% & 91.6\% & 0.74 & 24.4\% & 4.60 \\
grok-4-fast        & 248 & 80.8\% $\pm$ 2.6\% & 91.5\% & 0.70 & 13.7\% & 24.52 \\
claude-sonnet-4-5  & 240 & 79.3\% $\pm$ 2.4\% & 87.9\% & 0.73 & 30.8\% & 10.01 \\
claude-opus-4-5    & 200 & 79.3\% $\pm$ 3.5\% & 89.5\% & 0.73 & 18.0\% & 5.77 \\
grok-3-mini        & 278 & 78.6\% $\pm$ 2.4\% & 96.4\% & 0.75 & 24.8\% & 16.57 \\
claude-opus-4-1    & 184 & 75.4\% $\pm$ 3.5\% & 89.7\% & 0.73 & 27.7\% & --- \\
grok-4-1-f-r       & 90  & 75.3\% $\pm$ 4.3\% & 85.6\% & 0.73 & 23.3\% & 74.18 \\
gpt-5-mini         & 61  & 75.0\% $\pm$ 6.7\% & 88.5\% & 0.74 & 23.0\% & 22.19 \\
claude-haiku-4-5   & 92  & 74.2\% $\pm$ 4.7\% & 88.0\% & 0.73 & 25.0\% & 3.56 \\
gpt-4o             & 368 & 74.2\% $\pm$ 2.2\% & 90.2\% & 0.75 & 40.8\% & 4.16 \\
deepseek-chat      & 372 & 71.9\% $\pm$ 2.4\% & 95.4\% & 0.72 & 17.5\% & 8.96 \\
gpt-5.1            & 236 & 68.2\% $\pm$ 3.2\% & 83.5\% & 0.69 & 46.2\% & 4.31 \\
gpt-4              & 130 & 62.3\% $\pm$ 3.4\% & 90.8\% & 0.68 & 17.7\% & 4.30 \\
gpt-4o-mini        & 152 & 58.6\% $\pm$ 3.4\% & 81.6\% & 0.69 & 5.9\% & 7.71 \\
\hline
\end{tabular}
\label{tab1}
\end{center}
\end{table}

The leaderboard and its confidence intervals make three patterns visible. First, the Gemini 2.5 models lead in overall accuracy (85.5\% and 82.8\%), followed closely by Grok variants (grok-3 at 82.4\%, grok-4-fast at 80.8\%) which form the top tier with mean overall accuracy above 80\%. These models also exhibit high decision accuracy above 84\%, indicating that they almost always choose the correct reject/fail-to-reject action at $\alpha=0.05$. Second, Claude models (claude-sonnet-4-5, claude-opus-4-5, claude-opus-4-1) cluster in the 75--79\% overall accuracy range with consistently high decision accuracy (87--90\%), though claude-sonnet-4-5 shows an elevated hallucination rate of 30.8\%. Third, GPT models show more variability: gpt-4o achieves 74.2\% overall accuracy with 90.2\% decision accuracy but the highest hallucination rate (40.8\%), while gpt-5.1 trails at 68.2\% with a 46.2\% hallucination rate. The smaller gpt-4o-mini sits at the bottom with 58.6\% overall accuracy but notably the lowest hallucination rate (5.9\%), suggesting a trade-off between verbosity and factual grounding. DeepSeek Chat achieves a strong 95.4\% decision accuracy despite moderate overall accuracy (71.9\%), indicating reliable final decisions even when intermediate reasoning is imprecise.

\subsection{Family-Level Capability Profiles}

The radar panels group models into four families: GPT models, Grok models, Claude models, and a combined other family. Each radar traces five dimensions namely: test-method accuracy, decision accuracy, p-value accuracy, reasoning quality, and completeness normalized to $[0,1]$.

Within the GPT family radar, GPT-4o provides the strongest and most balanced profile, with high decision accuracy and reasoning quality but a noticeable dip in hallucination control compared to some Claude and Grok variants (consistent with Table~\ref{tab1}). GPT-5.1 and GPT-4o-mini achieve strong decision accuracy but exhibit more uneven coverage across dimensions, especially in p-value accuracy and completeness when prompts are less structured.

The Grok radar shows a consistently high and rounded footprint: both grok-4-fast and grok-3 score strongly on test-method selection, p-value accuracy, and decision correctness, giving them the broadest capability coverage of all families. The Claude radar is similarly strong but slightly more skewed toward reasoning quality and explanation completeness, reflecting that Claude often produces well-structured, interpretable arguments for its conclusions, even when slightly conservative on border-line decisions. In contrast, the Gemini/DeepSeek/other radar appears more irregular: DeepSeek Chat exhibits a reasonably solid decision and reasoning profile, while Gemini 2.5 variants display thinner radii, especially on test-method and p-value axes, indicating less stable statistical reasoning across the tested tasks.

\subsection{Error Patterns, Calibration, and Latency}

The statistical deep-dive tab combines a scatter plot of true versus predicted $p$-values with a mean absolute error (MAE) bar chart. For the top-tier Grok, Claude, and GPT-4o models, the scatter cloud concentrates near the diagonal $y=x$ line, indicating good $p$-value calibration and relatively small MAE bars. In contrast, Gemini variants and some smaller models show a more diffuse scatter with noticeable off-diagonal clusters, especially for mid-range $p$-values, which correspond to the most ambiguous cases for hypothesis testing. Decision errors in the scatter plots are concentrated around scenarios where the true $p$-value lies close to the significance threshold; here, models sometimes over-reject or over-conserve, depending on how they interpret borderline evidence.

The reasoning-quality box plots reveal that Grok, Claude, and GPT-4o maintain high median reasoning scores with tight interquartile ranges, indicating reliable coverage of key reasoning steps. Mid-tier and lower-tier models show both lower medians and heavier tails of low-scoring responses, consistent with occasional missing assumptions or incomplete conclusions. Finally, the latency bar chart shows that all models used in this study remain within a practically usable response time, with smaller models and some Gemini variants responding faster on average but at the cost of accuracy, and Grok/Claude/GPT-4o trading slightly higher latency for markedly better statistical performance. With Grok 4.1 fast reasoning having the highest latency.

\subsection{Qualitative Behaviour}

The qualitative inspector tab allows side-by-side inspection of full prompts, raw responses, and ground-truth statistics. These examples corroborate the quantitative error analysis: top-performing models typically select the correct test, compute a numerically reasonable statistic and $p$-value, and articulate a concise conclusion. Most hallucinations flagged by the dashboard are not wild fabrications but rather over-confident test-name claims, extra invented context, or inconsistent restatements of the same result. Lower-performing models occasionally mis-identify the test or conflate descriptive and inferential claims, leading to incorrect decisions even when the numerical computation is approximately right. Overall, the dashboard confirms that modern frontier models are capable of robust, end-to-end hypothesis testing when supported by appropriate prompting, but also highlights systematic weaknesses in test selection, calibration, and explanation that must be addressed before such systems can be trusted in high-stakes statistical settings.

\section{Conclusion \& Future Work}
This work establishes a rigorous benchmark for evaluating hypothesis testing in LLMs. We find that while top-tier models such as Grok-4, Claude Opus, and GPT-4o are approaching reliable performance on the tested $t$-test families, they still benefit significantly from structured prompting in particular few shot to guarantee response structure precision.

Future work will focus on:
\begin{itemize}
    \item Expanding the benchmark to include additional tests (ANOVA, regression, chi-square, non-parametric tests) and more varied effect-size and distributional regimes.
    \item Investigating agentic workflows where the model can iteratively request more data, inspect diagnostics, or run pilot simulations before committing to a decision.
    \item Refining hallucination and reasoning-quality metrics to better distinguish between harmless verbosity, genuine logical errors, and materially misleading claims.
\end{itemize}
Ultimately, this research paves the way for AI assistants that can participate more reliably in the scientific method, moving beyond text generation to evidence-based statistical reasoning and decision support.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}