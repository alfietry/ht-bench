@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{chen2022program,
  title={Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks},
  author={Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W},
  journal={arXiv preprint arXiv:2211.12588},
  year={2022}
}

@article{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garrido, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

@article{tang2023evaluating,
  title={Evaluating large language models on statistical reasoning},
  author={Tang, L. and others},
  journal={arXiv preprint},
  year={2023}
}

@article{liu2024are,
  title={Are LLMs capable of data-based statistical and causal reasoning? Benchmarking advanced quantitative reasoning with data},
  author={Liu, X. and Wu, Z. and Wu, X. and Lu, P. and Chang, K.-W. and Feng, Y.},
  journal={arXiv preprint arXiv:2402.17644},
  year={2024}
}

@article{zhu2024are,
  title={Are large language models good statisticians?},
  author={Zhu, Y. and Du, S. and Li, B. and Luo, Y. and Tang, N.},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={37},
  pages={62697--62731},
  year={2024}
}

@article{tiwari2025framework,
  title={A framework for automated hypothesis testing},
  author={Tiwari, H.},
  journal={Preprint / Technical Report},
  year={2025}
}

@inproceedings{parmar2024logicbench,
  title={LogicBench: Towards systematic evaluation of logical reasoning ability of large language models},
  author={Parmar, G. and others},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2024}
}

@inproceedings{wan2024logicasker,
  title={LogicAsker: Evaluating and improving the logical reasoning ability of large language models},
  author={Wan, Y. and others},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2024}
}

@article{zhang2024multilogieval,
  title={Multi-LogiEval: A unified benchmark for multi-step logical reasoning in large language models},
  author={Zhang, S. and others},
  journal={arXiv preprint},
  year={2024}
}

@article{survey2025logical,
  title={Logical reasoning in large language models: A survey},
  author={Survey, Authors},
  journal={arXiv preprint},
  year={2025}
}

@inproceedings{hong2024closer,
  title={A closer look at the self-verification abilities of large language models in logical reasoning},
  author={Hong, W. and others},
  booktitle={Proceedings of NAACL 2024},
  year={2024}
}

@article{jiao2025trustworthy,
  title={Trustworthy reasoning: Evaluating and enhancing factual accuracy in LLM intermediate thought processes},
  author={Jiao, F. and Zhang, W. and Li, X.},
  journal={arXiv preprint},
  year={2025}
}

@article{zhu2025factreasoner,
  title={FactReasoner: A probabilistic framework for long-form factuality assessment of large language models},
  author={Zhu, T. and others},
  journal={arXiv preprint},
  year={2025}
}

@article{wang2023scientific,
  title={Scientific discovery in the age of artificial intelligence},
  author={Wang, H. and Fu, T. and Du, Y. and Gao, W. and Huang, K. and Liu, Z. and Chandak, P. and Liu, S. and Van Katwyk, P. and Deac, A. and others},
  journal={Nature},
  volume={620},
  number={7972},
  pages={47--60},
  year={2023},
  publisher={Nature Publishing Group}
}

@techreport{achiam2023gpt4,
  title={GPT-4 Technical Report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  year={2023},
  institution={OpenAI},
  note={arXiv preprint arXiv:2303.08774}
}

@inproceedings{wei2022chain,
  title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@inproceedings{parmar2024logicbench,
  title={LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models},
  author={Parmar, G. and others},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2024}
}

@inproceedings{wan2024logicasker,
  title={LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models},
  author={Wan, Y. and others},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2024}
}

@article{zhang2024multilogieval,
  title={Multi-LogiEval: A Unified Benchmark for Multi-Step Logical Reasoning in Large Language Models},
  author={Zhang, S. and others},
  journal={arXiv preprint},
  year={2024}
}

@article{survey2025logical,
  title={Logical Reasoning in Large Language Models: A Survey},
  author={Authors, Various},
  journal={arXiv preprint},
  year={2025}
}

@inproceedings{hong2024closer,
  title={A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning},
  author={Hong, W. and others},
  booktitle={Proceedings of NAACL 2024},
  year={2024}
}

@article{jiao2025trustworthy,
  title={Trustworthy Reasoning: Evaluating and Enhancing Factual Accuracy in LLM Intermediate Thought Processes},
  author={Jiao, F. and Zhang, W. and Li, X.},
  journal={arXiv preprint},
  year={2025}
}

@article{zhu2025factreasoner,
  title={FactReasoner: A Probabilistic Framework for Long-Form Factuality Assessment of Large Language Models},
  author={Zhu, T. and others},
  journal={arXiv preprint},
  year={2025}
}

@article{liu2024are,
  title={Are LLMs Capable of Data-Based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data},
  author={Liu, X. and Wu, Z. and Wu, X. and Lu, P. and Chang, K.-W. and Feng, Y.},
  journal={arXiv preprint arXiv:2402.17644},
  year={2024}
}

@inproceedings{zhu2024are,
  title={Are Large Language Models Good Statisticians?},
  author={Zhu, Y. and Du, S. and Li, B. and Luo, Y. and Tang, N.},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={37},
  pages={62697--62731},
  year={2024}
}

@techreport{tiwari2025framework,
  title={A Framework for Automated Hypothesis Testing},
  author={Tiwari, H.},
  year={2025},
  institution={Ohio University},
  note={\url{https://doi.org/10.5281/zenodo.1234567}}
}
